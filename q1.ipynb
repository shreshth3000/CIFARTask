{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-10-04T07:32:15.437121Z",
     "iopub.status.busy": "2025-10-04T07:32:15.436213Z",
     "iopub.status.idle": "2025-10-04T07:32:15.441693Z",
     "shell.execute_reply": "2025-10-04T07:32:15.440842Z",
     "shell.execute_reply.started": "2025-10-04T07:32:15.437088Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.optim import AdamW\n",
    "import torchvision.transforms.v2 as v2\n",
    "from torchvision.datasets import CIFAR10\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "from torchvision.transforms import RandAugment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-04T07:32:15.950716Z",
     "iopub.status.busy": "2025-10-04T07:32:15.950490Z",
     "iopub.status.idle": "2025-10-04T07:32:15.954404Z",
     "shell.execute_reply": "2025-10-04T07:32:15.953661Z",
     "shell.execute_reply.started": "2025-10-04T07:32:15.950697Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "device =\"cuda\" if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-04T07:32:16.094775Z",
     "iopub.status.busy": "2025-10-04T07:32:16.094201Z",
     "iopub.status.idle": "2025-10-04T07:32:16.099562Z",
     "shell.execute_reply": "2025-10-04T07:32:16.098725Z",
     "shell.execute_reply.started": "2025-10-04T07:32:16.094757Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_transformation = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding =4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.05),\n",
    "    #RandAugment(num_ops=3, magnitude=10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.491, 0.482, 0.447),(0.247, 0.243, 0.261))\n",
    "])\n",
    "\n",
    "test_transformation = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.491, 0.482, 0.447),(0.247, 0.243, 0.261))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-04T07:32:16.382219Z",
     "iopub.status.busy": "2025-10-04T07:32:16.381568Z",
     "iopub.status.idle": "2025-10-04T07:32:16.386274Z",
     "shell.execute_reply": "2025-10-04T07:32:16.385385Z",
     "shell.execute_reply.started": "2025-10-04T07:32:16.382193Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_classes=10\n",
    "img_size = 32\n",
    "patch_size = 4\n",
    "in_chans = 3\n",
    "embed_dim = 384\n",
    "depth = 6\n",
    "num_heads = 12\n",
    "mlp_ratio = 4.0\n",
    "drop_rate = 0.1\n",
    "epochs = 200\n",
    "lr = 9e-4\n",
    "weight_decay = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-04T07:32:16.636487Z",
     "iopub.status.busy": "2025-10-04T07:32:16.635966Z",
     "iopub.status.idle": "2025-10-04T07:32:16.641969Z",
     "shell.execute_reply": "2025-10-04T07:32:16.641345Z",
     "shell.execute_reply.started": "2025-10-04T07:32:16.636463Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "cutmix = v2.CutMix(num_classes=num_classes)\n",
    "mixup = v2.MixUp(num_classes=num_classes)\n",
    "mix_transform = v2.RandomChoice([cutmix, mixup])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-04T07:32:16.900904Z",
     "iopub.status.busy": "2025-10-04T07:32:16.900694Z",
     "iopub.status.idle": "2025-10-04T07:32:18.496323Z",
     "shell.execute_reply": "2025-10-04T07:32:18.495724Z",
     "shell.execute_reply.started": "2025-10-04T07:32:16.900887Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_dataset = CIFAR10(root='./data', train=True, download=True, transform= train_transformation)\n",
    "test_dataset  = CIFAR10(root='./data', train=False, download=True, transform=test_transformation)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-04T07:32:18.497851Z",
     "iopub.status.busy": "2025-10-04T07:32:18.497599Z",
     "iopub.status.idle": "2025-10-04T07:32:18.501731Z",
     "shell.execute_reply": "2025-10-04T07:32:18.500901Z",
     "shell.execute_reply.started": "2025-10-04T07:32:18.497830Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "steps_per_epoch = len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-04T07:32:18.502693Z",
     "iopub.status.busy": "2025-10-04T07:32:18.502489Z",
     "iopub.status.idle": "2025-10-04T07:32:18.516075Z",
     "shell.execute_reply": "2025-10-04T07:32:18.515494Z",
     "shell.execute_reply.started": "2025-10-04T07:32:18.502670Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class PatchEmbed(nn.Module):\n",
    "    def __init__(self, img_size=32, patch_size=4, in_chans=3, embed_dim=256):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.n_patches = (img_size // patch_size) ** 2\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-04T07:32:19.327624Z",
     "iopub.status.busy": "2025-10-04T07:32:19.327400Z",
     "iopub.status.idle": "2025-10-04T07:32:19.332873Z",
     "shell.execute_reply": "2025-10-04T07:32:19.332061Z",
     "shell.execute_reply.started": "2025-10-04T07:32:19.327608Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, drop=0.0):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = nn.GELU()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-04T07:32:19.918355Z",
     "iopub.status.busy": "2025-10-04T07:32:19.918119Z",
     "iopub.status.idle": "2025-10-04T07:32:19.923643Z",
     "shell.execute_reply": "2025-10-04T07:32:19.922885Z",
     "shell.execute_reply.started": "2025-10-04T07:32:19.918339Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class TokenMixer(nn.Module):\n",
    "    def __init__(self, embed_dim, mix_type='avg'):\n",
    "        super().__init__()\n",
    "        if mix_type == 'conv':\n",
    "            self.mix = nn.Conv1d(embed_dim, embed_dim, kernel_size=3, padding=1, groups=embed_dim)\n",
    "        else:\n",
    "            self.mix = lambda x: x.mean(dim=1, keepdim=True).repeat(1, x.size(1), 1)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        if hasattr(self, 'mix'):\n",
    "            x_mixed = self.mix(x.transpose(1,2)).transpose(1,2) if isinstance(self.mix, nn.Conv1d) else self.mix(x)\n",
    "            return x + x_mixed\n",
    "        else:\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-04T07:32:20.478312Z",
     "iopub.status.busy": "2025-10-04T07:32:20.477767Z",
     "iopub.status.idle": "2025-10-04T07:32:20.483985Z",
     "shell.execute_reply": "2025-10-04T07:32:20.482967Z",
     "shell.execute_reply.started": "2025-10-04T07:32:20.478284Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, dim, num_heads, mlp_ratio=4.0, drop=0.0, token_mixing=False):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.attn = nn.MultiheadAttention(embed_dim=dim, num_heads=num_heads, batch_first=True, dropout=drop)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        self.mlp = MLP(in_features=dim, hidden_features=int(dim*mlp_ratio), drop=drop)\n",
    "        self.token_mixer = TokenMixer(dim) if token_mixing else nn.Identity() \n",
    "\n",
    "    def forward(self, x):\n",
    "        x_attn = self.norm1(x)\n",
    "        attn_out, _ = self.attn(x_attn, x_attn, x_attn, need_weights=False)\n",
    "        x = x + attn_out\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        x = self.token_mixer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-04T07:32:21.117477Z",
     "iopub.status.busy": "2025-10-04T07:32:21.116986Z",
     "iopub.status.idle": "2025-10-04T07:32:21.126556Z",
     "shell.execute_reply": "2025-10-04T07:32:21.125854Z",
     "shell.execute_reply.started": "2025-10-04T07:32:21.117456Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ViT(nn.Module):\n",
    "    def __init__(self, *, img_size=32, patch_size=4, in_chans=3, num_classes=10,\n",
    "                 embed_dim=256, depth=6, num_heads=8, mlp_ratio=4.0, drop_rate=0.0):\n",
    "        super().__init__()\n",
    "        self.patch_embed = PatchEmbed(img_size=img_size, patch_size=patch_size,\n",
    "                                      in_chans=in_chans, embed_dim=embed_dim)\n",
    "        n_patches = self.patch_embed.n_patches\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, n_patches + 1, embed_dim))\n",
    "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, drop=drop_rate)\n",
    "            for _ in range(depth)\n",
    "        ])\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        self.head = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
    "        nn.init.trunc_normal_(self.cls_token, std=0.02)\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.trunc_normal_(m.weight, std=0.02)\n",
    "            if m.bias is not None:\n",
    "                nn.init.zeros_(m.bias)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.ones_(m.weight)\n",
    "            nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B = x.shape[0]\n",
    "        x = self.patch_embed(x)\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        x = x + self.pos_embed\n",
    "        x = self.pos_drop(x)\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "        x = self.norm(x)\n",
    "        cls = x[:, 0]\n",
    "        out = self.head(cls)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-04T07:32:21.752574Z",
     "iopub.status.busy": "2025-10-04T07:32:21.752328Z",
     "iopub.status.idle": "2025-10-04T07:32:21.917411Z",
     "shell.execute_reply": "2025-10-04T07:32:21.916827Z",
     "shell.execute_reply.started": "2025-10-04T07:32:21.752556Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model = ViT(img_size=img_size, patch_size=patch_size, in_chans=in_chans, num_classes=num_classes,\n",
    "            embed_dim=embed_dim, depth=depth, num_heads=num_heads, mlp_ratio=mlp_ratio, drop_rate=drop_rate)\n",
    "model = torch.nn.DataParallel(model)\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "optimizer = AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "total_steps =epochs*steps_per_epoch\n",
    "warmup_steps = int(0.15 * total_steps)  \n",
    "lr_scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=warmup_steps,\n",
    "    num_training_steps=total_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-04T07:32:22.416847Z",
     "iopub.status.busy": "2025-10-04T07:32:22.416096Z",
     "iopub.status.idle": "2025-10-04T07:32:22.424067Z",
     "shell.execute_reply": "2025-10-04T07:32:22.423171Z",
     "shell.execute_reply.started": "2025-10-04T07:32:22.416822Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch, lr_schduler):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for i, (imgs, targets) in enumerate(train_loader):\n",
    "        imgs, targets = imgs.to(device), targets.to(device)\n",
    "        #imgs,targets = mix_transform(imgs, targets)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(imgs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * imgs.size(0)\n",
    "        if targets.ndim == 1:\n",
    "            _, preds = outputs.max(1)\n",
    "            correct += preds.eq(targets).sum().item()\n",
    "        else:\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            targets_indices = targets.argmax(dim=1)\n",
    "            correct += preds.eq(targets_indices).sum().item()\n",
    "        total += imgs.size(0)  \n",
    "    acc = 100.0 * correct / total\n",
    "    lr_scheduler.step()\n",
    "    print(f\"Epoch {epoch}: Train Loss {running_loss/total:.4f} Acc {acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-04T07:32:23.169636Z",
     "iopub.status.busy": "2025-10-04T07:32:23.169336Z",
     "iopub.status.idle": "2025-10-04T07:32:23.175094Z",
     "shell.execute_reply": "2025-10-04T07:32:23.174264Z",
     "shell.execute_reply.started": "2025-10-04T07:32:23.169615Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def evaluate():\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    loss_sum = 0.0\n",
    "    with torch.no_grad():\n",
    "        for imgs, targets in test_loader:\n",
    "            imgs, targets = imgs.to(device), targets.to(device)\n",
    "            outputs = model(imgs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss_sum += loss.item() * imgs.size(0)\n",
    "            _, preds = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += preds.eq(targets).sum().item()\n",
    "    acc = 100.0 * correct / total\n",
    "    print(f\"Test Loss {loss_sum/total:.4f} Acc {acc:.2f}%\")\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-04T07:32:23.812385Z",
     "iopub.status.busy": "2025-10-04T07:32:23.812131Z",
     "iopub.status.idle": "2025-10-04T10:42:10.324028Z",
     "shell.execute_reply": "2025-10-04T10:42:10.323025Z",
     "shell.execute_reply.started": "2025-10-04T07:32:23.812368Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss 2.3543 Acc 12.94%\n",
      "Test Loss 2.3364 Acc 13.24%\n",
      "Epoch 1 complete...\n",
      "Epoch 2: Train Loss 2.3168 Acc 13.48%\n",
      "Test Loss 2.2326 Acc 17.30%\n",
      "Epoch 2 complete...\n",
      "Epoch 3: Train Loss 2.2648 Acc 15.46%\n",
      "Test Loss 2.1536 Acc 22.48%\n",
      "Epoch 3 complete...\n",
      "Epoch 4: Train Loss 2.2287 Acc 17.84%\n",
      "Test Loss 2.1114 Acc 23.28%\n",
      "Epoch 4 complete...\n",
      "Epoch 5: Train Loss 2.2080 Acc 18.57%\n",
      "Test Loss 2.0864 Acc 24.97%\n",
      "Epoch 5 complete...\n",
      "Epoch 6: Train Loss 2.1901 Acc 19.36%\n",
      "Test Loss 2.0691 Acc 26.24%\n",
      "Epoch 6 complete...\n",
      "Epoch 7: Train Loss 2.1773 Acc 20.35%\n",
      "Test Loss 2.0565 Acc 26.65%\n",
      "Epoch 7 complete...\n",
      "Epoch 8: Train Loss 2.1614 Acc 20.86%\n",
      "Test Loss 2.0342 Acc 27.78%\n",
      "Epoch 8 complete...\n",
      "Epoch 9: Train Loss 2.1434 Acc 21.85%\n",
      "Test Loss 2.0120 Acc 28.83%\n",
      "Epoch 9 complete...\n",
      "Epoch 10: Train Loss 2.1285 Acc 22.45%\n",
      "Test Loss 1.9912 Acc 29.95%\n",
      "Epoch 10 complete...\n",
      "Epoch 11: Train Loss 2.1075 Acc 23.31%\n",
      "Test Loss 1.9840 Acc 30.00%\n",
      "Epoch 11 complete...\n",
      "Epoch 12: Train Loss 2.0976 Acc 23.71%\n",
      "Test Loss 1.9648 Acc 31.34%\n",
      "Epoch 12 complete...\n",
      "Epoch 13: Train Loss 2.0879 Acc 24.50%\n",
      "Test Loss 1.9622 Acc 31.94%\n",
      "Epoch 13 complete...\n",
      "Epoch 14: Train Loss 2.0768 Acc 25.03%\n",
      "Test Loss 1.9432 Acc 32.73%\n",
      "Epoch 14 complete...\n",
      "Epoch 15: Train Loss 2.0610 Acc 25.76%\n",
      "Test Loss 1.9405 Acc 32.91%\n",
      "Epoch 15 complete...\n",
      "Epoch 16: Train Loss 2.0532 Acc 26.31%\n",
      "Test Loss 1.9160 Acc 33.59%\n",
      "Epoch 16 complete...\n",
      "Epoch 17: Train Loss 2.0386 Acc 27.00%\n",
      "Test Loss 1.9087 Acc 33.26%\n",
      "Epoch 17 complete...\n",
      "Epoch 18: Train Loss 2.0298 Acc 27.51%\n",
      "Test Loss 1.9015 Acc 34.50%\n",
      "Epoch 18 complete...\n",
      "Epoch 19: Train Loss 2.0160 Acc 28.34%\n",
      "Test Loss 1.9015 Acc 34.67%\n",
      "Epoch 19 complete...\n",
      "Epoch 20: Train Loss 2.0041 Acc 29.20%\n",
      "Test Loss 1.8808 Acc 35.02%\n",
      "Epoch 20 complete...\n",
      "Epoch 21: Train Loss 1.9913 Acc 29.53%\n",
      "Test Loss 1.8669 Acc 36.27%\n",
      "Epoch 21 complete...\n",
      "Epoch 22: Train Loss 1.9799 Acc 29.92%\n",
      "Test Loss 1.8482 Acc 37.24%\n",
      "Epoch 22 complete...\n",
      "Epoch 23: Train Loss 1.9689 Acc 30.79%\n",
      "Test Loss 1.8273 Acc 38.27%\n",
      "Epoch 23 complete...\n",
      "Epoch 24: Train Loss 1.9586 Acc 31.22%\n",
      "Test Loss 1.8216 Acc 38.48%\n",
      "Epoch 24 complete...\n",
      "Epoch 25: Train Loss 1.9478 Acc 31.91%\n",
      "Test Loss 1.7912 Acc 39.47%\n",
      "Epoch 25 complete...\n",
      "Epoch 26: Train Loss 1.9388 Acc 32.57%\n",
      "Test Loss 1.8018 Acc 39.40%\n",
      "Epoch 26 complete...\n",
      "Epoch 27: Train Loss 1.9292 Acc 33.08%\n",
      "Test Loss 1.7895 Acc 39.61%\n",
      "Epoch 27 complete...\n",
      "Epoch 28: Train Loss 1.9153 Acc 33.78%\n",
      "Test Loss 1.7587 Acc 41.22%\n",
      "Epoch 28 complete...\n",
      "Epoch 29: Train Loss 1.9133 Acc 33.94%\n",
      "Test Loss 1.7549 Acc 41.71%\n",
      "Epoch 29 complete...\n",
      "Epoch 30: Train Loss 1.9001 Acc 34.32%\n",
      "Test Loss 1.7499 Acc 42.57%\n",
      "Epoch 30 complete...\n",
      "Epoch 31: Train Loss 1.8942 Acc 34.66%\n",
      "Test Loss 1.7329 Acc 42.67%\n",
      "Epoch 31 complete...\n",
      "Epoch 32: Train Loss 1.8819 Acc 35.27%\n",
      "Test Loss 1.7309 Acc 42.95%\n",
      "Epoch 32 complete...\n",
      "Epoch 33: Train Loss 1.8738 Acc 35.83%\n",
      "Test Loss 1.7023 Acc 43.82%\n",
      "Epoch 33 complete...\n",
      "Epoch 34: Train Loss 1.8631 Acc 36.10%\n",
      "Test Loss 1.6933 Acc 44.69%\n",
      "Epoch 34 complete...\n",
      "Epoch 35: Train Loss 1.8573 Acc 36.84%\n",
      "Test Loss 1.6882 Acc 44.93%\n",
      "Epoch 35 complete...\n",
      "Epoch 36: Train Loss 1.8466 Acc 37.22%\n",
      "Test Loss 1.6782 Acc 45.82%\n",
      "Epoch 36 complete...\n",
      "Epoch 37: Train Loss 1.8381 Acc 37.43%\n",
      "Test Loss 1.6837 Acc 45.55%\n",
      "Epoch 37 complete...\n",
      "Epoch 38: Train Loss 1.8309 Acc 37.89%\n",
      "Test Loss 1.6728 Acc 45.97%\n",
      "Epoch 38 complete...\n",
      "Epoch 39: Train Loss 1.8180 Acc 38.45%\n",
      "Test Loss 1.6550 Acc 46.44%\n",
      "Epoch 39 complete...\n",
      "Epoch 40: Train Loss 1.8110 Acc 38.95%\n",
      "Test Loss 1.6562 Acc 46.48%\n",
      "Epoch 40 complete...\n",
      "Epoch 41: Train Loss 1.8038 Acc 39.11%\n",
      "Test Loss 1.6359 Acc 47.49%\n",
      "Epoch 41 complete...\n",
      "Epoch 42: Train Loss 1.7923 Acc 39.96%\n",
      "Test Loss 1.6175 Acc 49.07%\n",
      "Epoch 42 complete...\n",
      "Epoch 43: Train Loss 1.7846 Acc 40.44%\n",
      "Test Loss 1.6238 Acc 48.92%\n",
      "Epoch 43 complete...\n",
      "Epoch 44: Train Loss 1.7785 Acc 40.57%\n",
      "Test Loss 1.6122 Acc 49.04%\n",
      "Epoch 44 complete...\n",
      "Epoch 45: Train Loss 1.7689 Acc 41.18%\n",
      "Test Loss 1.6156 Acc 49.01%\n",
      "Epoch 45 complete...\n",
      "Epoch 46: Train Loss 1.7603 Acc 41.74%\n",
      "Test Loss 1.5987 Acc 49.93%\n",
      "Epoch 46 complete...\n",
      "Epoch 47: Train Loss 1.7541 Acc 41.64%\n",
      "Test Loss 1.6138 Acc 48.61%\n",
      "Epoch 47 complete...\n",
      "Epoch 48: Train Loss 1.7488 Acc 41.89%\n",
      "Test Loss 1.5956 Acc 50.20%\n",
      "Epoch 48 complete...\n",
      "Epoch 49: Train Loss 1.7351 Acc 42.82%\n",
      "Test Loss 1.6156 Acc 49.52%\n",
      "Epoch 49 complete...\n",
      "Epoch 50: Train Loss 1.7306 Acc 43.15%\n",
      "Test Loss 1.5815 Acc 50.61%\n",
      "Epoch 50 complete...\n",
      "Epoch 51: Train Loss 1.7261 Acc 43.14%\n",
      "Test Loss 1.5700 Acc 51.27%\n",
      "Epoch 51 complete...\n",
      "Epoch 52: Train Loss 1.7177 Acc 43.87%\n",
      "Test Loss 1.5754 Acc 50.91%\n",
      "Epoch 52 complete...\n",
      "Epoch 53: Train Loss 1.7033 Acc 44.36%\n",
      "Test Loss 1.5556 Acc 51.87%\n",
      "Epoch 53 complete...\n",
      "Epoch 54: Train Loss 1.7040 Acc 44.26%\n",
      "Test Loss 1.5678 Acc 51.40%\n",
      "Epoch 54 complete...\n",
      "Epoch 55: Train Loss 1.6973 Acc 44.68%\n",
      "Test Loss 1.5461 Acc 52.17%\n",
      "Epoch 55 complete...\n",
      "Epoch 56: Train Loss 1.6920 Acc 45.23%\n",
      "Test Loss 1.5508 Acc 52.26%\n",
      "Epoch 56 complete...\n",
      "Epoch 57: Train Loss 1.6792 Acc 45.52%\n",
      "Test Loss 1.5301 Acc 53.03%\n",
      "Epoch 57 complete...\n",
      "Epoch 58: Train Loss 1.6766 Acc 45.91%\n",
      "Test Loss 1.5186 Acc 53.50%\n",
      "Epoch 58 complete...\n",
      "Epoch 59: Train Loss 1.6688 Acc 46.06%\n",
      "Test Loss 1.5144 Acc 53.59%\n",
      "Epoch 59 complete...\n",
      "Epoch 60: Train Loss 1.6599 Acc 46.81%\n",
      "Test Loss 1.5295 Acc 52.60%\n",
      "Epoch 60 complete...\n",
      "Epoch 61: Train Loss 1.6533 Acc 46.80%\n",
      "Test Loss 1.5105 Acc 53.62%\n",
      "Epoch 61 complete...\n",
      "Epoch 62: Train Loss 1.6504 Acc 47.23%\n",
      "Test Loss 1.5072 Acc 54.23%\n",
      "Epoch 62 complete...\n",
      "Epoch 63: Train Loss 1.6469 Acc 47.05%\n",
      "Test Loss 1.5321 Acc 53.41%\n",
      "Epoch 63 complete...\n",
      "Epoch 64: Train Loss 1.6387 Acc 47.69%\n",
      "Test Loss 1.4768 Acc 55.19%\n",
      "Epoch 64 complete...\n",
      "Epoch 65: Train Loss 1.6317 Acc 47.73%\n",
      "Test Loss 1.4964 Acc 54.61%\n",
      "Epoch 65 complete...\n",
      "Epoch 66: Train Loss 1.6273 Acc 48.01%\n",
      "Test Loss 1.5038 Acc 54.89%\n",
      "Epoch 66 complete...\n",
      "Epoch 67: Train Loss 1.6220 Acc 48.40%\n",
      "Test Loss 1.4657 Acc 56.12%\n",
      "Epoch 67 complete...\n",
      "Epoch 68: Train Loss 1.6176 Acc 48.62%\n",
      "Test Loss 1.4636 Acc 56.51%\n",
      "Epoch 68 complete...\n",
      "Epoch 69: Train Loss 1.6124 Acc 49.09%\n",
      "Test Loss 1.4813 Acc 55.21%\n",
      "Epoch 69 complete...\n",
      "Epoch 70: Train Loss 1.6089 Acc 49.01%\n",
      "Test Loss 1.4448 Acc 57.18%\n",
      "Epoch 70 complete...\n",
      "Epoch 71: Train Loss 1.5999 Acc 49.36%\n",
      "Test Loss 1.4529 Acc 56.59%\n",
      "Epoch 71 complete...\n",
      "Epoch 72: Train Loss 1.5940 Acc 49.82%\n",
      "Test Loss 1.4374 Acc 57.71%\n",
      "Epoch 72 complete...\n",
      "Epoch 73: Train Loss 1.5909 Acc 49.92%\n",
      "Test Loss 1.4413 Acc 57.38%\n",
      "Epoch 73 complete...\n",
      "Epoch 74: Train Loss 1.5869 Acc 50.21%\n",
      "Test Loss 1.4275 Acc 57.80%\n",
      "Epoch 74 complete...\n",
      "Epoch 75: Train Loss 1.5793 Acc 50.52%\n",
      "Test Loss 1.4138 Acc 58.35%\n",
      "Epoch 75 complete...\n",
      "Epoch 76: Train Loss 1.5800 Acc 50.42%\n",
      "Test Loss 1.4076 Acc 59.04%\n",
      "Epoch 76 complete...\n",
      "Epoch 77: Train Loss 1.5683 Acc 50.97%\n",
      "Test Loss 1.3972 Acc 59.48%\n",
      "Epoch 77 complete...\n",
      "Epoch 78: Train Loss 1.5631 Acc 51.22%\n",
      "Test Loss 1.3810 Acc 59.74%\n",
      "Epoch 78 complete...\n",
      "Epoch 79: Train Loss 1.5614 Acc 51.46%\n",
      "Test Loss 1.3951 Acc 59.83%\n",
      "Epoch 79 complete...\n",
      "Epoch 80: Train Loss 1.5547 Acc 51.54%\n",
      "Test Loss 1.3828 Acc 60.28%\n",
      "Epoch 80 complete...\n",
      "Epoch 81: Train Loss 1.5452 Acc 51.95%\n",
      "Test Loss 1.3656 Acc 61.04%\n",
      "Epoch 81 complete...\n",
      "Epoch 82: Train Loss 1.5436 Acc 52.17%\n",
      "Test Loss 1.3783 Acc 60.40%\n",
      "Epoch 82 complete...\n",
      "Epoch 83: Train Loss 1.5431 Acc 52.23%\n",
      "Test Loss 1.3604 Acc 61.10%\n",
      "Epoch 83 complete...\n",
      "Epoch 84: Train Loss 1.5335 Acc 52.66%\n",
      "Test Loss 1.3523 Acc 61.06%\n",
      "Epoch 84 complete...\n",
      "Epoch 85: Train Loss 1.5293 Acc 52.73%\n",
      "Test Loss 1.3599 Acc 61.39%\n",
      "Epoch 85 complete...\n",
      "Epoch 86: Train Loss 1.5265 Acc 52.91%\n",
      "Test Loss 1.3676 Acc 60.48%\n",
      "Epoch 86 complete...\n",
      "Epoch 87: Train Loss 1.5175 Acc 53.38%\n",
      "Test Loss 1.3358 Acc 62.03%\n",
      "Epoch 87 complete...\n",
      "Epoch 88: Train Loss 1.5177 Acc 53.55%\n",
      "Test Loss 1.3318 Acc 62.50%\n",
      "Epoch 88 complete...\n",
      "Epoch 97: Train Loss 1.4750 Acc 55.62%\n",
      "Test Loss 1.3085 Acc 63.24%\n",
      "Epoch 97 complete...\n",
      "Epoch 98: Train Loss 1.4682 Acc 55.66%\n",
      "Test Loss 1.2785 Acc 64.81%\n",
      "Epoch 98 complete...\n",
      "Epoch 99: Train Loss 1.4619 Acc 55.94%\n",
      "Test Loss 1.2789 Acc 64.70%\n",
      "Epoch 99 complete...\n",
      "Epoch 100: Train Loss 1.4657 Acc 55.95%\n",
      "Test Loss 1.3263 Acc 63.24%\n",
      "Epoch 100 complete...\n",
      "Epoch 101: Train Loss 1.4602 Acc 56.17%\n",
      "Test Loss 1.2726 Acc 65.88%\n",
      "Epoch 101 complete...\n",
      "Epoch 102: Train Loss 1.4547 Acc 56.30%\n",
      "Test Loss 1.2833 Acc 64.20%\n",
      "Epoch 102 complete...\n",
      "Epoch 103: Train Loss 1.4480 Acc 56.79%\n",
      "Test Loss 1.2504 Acc 66.15%\n",
      "Epoch 103 complete...\n",
      "Epoch 104: Train Loss 1.4488 Acc 56.64%\n",
      "Test Loss 1.2837 Acc 64.43%\n",
      "Epoch 104 complete...\n",
      "Epoch 105: Train Loss 1.4385 Acc 57.24%\n",
      "Test Loss 1.2590 Acc 66.37%\n",
      "Epoch 105 complete...\n",
      "Epoch 106: Train Loss 1.4366 Acc 57.38%\n",
      "Test Loss 1.2361 Acc 66.39%\n",
      "Epoch 106 complete...\n",
      "Epoch 107: Train Loss 1.4348 Acc 57.41%\n",
      "Test Loss 1.2479 Acc 66.08%\n",
      "Epoch 107 complete...\n",
      "Epoch 108: Train Loss 1.4290 Acc 57.81%\n",
      "Test Loss 1.2203 Acc 67.53%\n",
      "Epoch 108 complete...\n",
      "Epoch 109: Train Loss 1.4276 Acc 57.90%\n",
      "Test Loss 1.2401 Acc 66.67%\n",
      "Epoch 109 complete...\n",
      "Epoch 110: Train Loss 1.4221 Acc 58.01%\n",
      "Test Loss 1.2448 Acc 66.73%\n",
      "Epoch 110 complete...\n",
      "Epoch 111: Train Loss 1.4191 Acc 58.32%\n",
      "Test Loss 1.2278 Acc 67.05%\n",
      "Epoch 111 complete...\n",
      "Epoch 112: Train Loss 1.4135 Acc 58.20%\n",
      "Test Loss 1.2146 Acc 67.72%\n",
      "Epoch 112 complete...\n",
      "Epoch 113: Train Loss 1.4098 Acc 58.52%\n",
      "Test Loss 1.2177 Acc 67.54%\n",
      "Epoch 113 complete...\n",
      "Epoch 114: Train Loss 1.4053 Acc 58.88%\n",
      "Test Loss 1.2194 Acc 67.74%\n",
      "Epoch 114 complete...\n",
      "Epoch 115: Train Loss 1.4014 Acc 59.08%\n",
      "Test Loss 1.1966 Acc 68.61%\n",
      "Epoch 115 complete...\n",
      "Epoch 116: Train Loss 1.3970 Acc 59.07%\n",
      "Test Loss 1.2156 Acc 68.16%\n",
      "Epoch 116 complete...\n",
      "Epoch 117: Train Loss 1.3935 Acc 59.21%\n",
      "Test Loss 1.2191 Acc 67.48%\n",
      "Epoch 117 complete...\n",
      "Epoch 118: Train Loss 1.3908 Acc 59.40%\n",
      "Test Loss 1.2228 Acc 67.47%\n",
      "Epoch 118 complete...\n",
      "Epoch 119: Train Loss 1.3886 Acc 59.67%\n",
      "Test Loss 1.2014 Acc 68.44%\n",
      "Epoch 119 complete...\n",
      "Epoch 120: Train Loss 1.3832 Acc 60.03%\n",
      "Test Loss 1.1983 Acc 68.86%\n",
      "Epoch 120 complete...\n",
      "Epoch 121: Train Loss 1.3826 Acc 59.71%\n",
      "Test Loss 1.2196 Acc 67.83%\n",
      "Epoch 121 complete...\n",
      "Epoch 122: Train Loss 1.3750 Acc 60.22%\n",
      "Test Loss 1.1843 Acc 68.91%\n",
      "Epoch 122 complete...\n",
      "Epoch 123: Train Loss 1.3755 Acc 59.93%\n",
      "Test Loss 1.1749 Acc 69.84%\n",
      "Epoch 123 complete...\n",
      "Epoch 124: Train Loss 1.3679 Acc 60.46%\n",
      "Test Loss 1.1997 Acc 68.92%\n",
      "Epoch 124 complete...\n",
      "Epoch 125: Train Loss 1.3651 Acc 60.75%\n",
      "Test Loss 1.1780 Acc 69.10%\n",
      "Epoch 125 complete...\n",
      "Epoch 126: Train Loss 1.3637 Acc 60.72%\n",
      "Test Loss 1.2002 Acc 68.70%\n",
      "Epoch 126 complete...\n",
      "Epoch 127: Train Loss 1.3600 Acc 60.95%\n",
      "Test Loss 1.1645 Acc 69.62%\n",
      "Epoch 127 complete...\n",
      "Epoch 128: Train Loss 1.3584 Acc 61.03%\n",
      "Test Loss 1.1553 Acc 71.00%\n",
      "Epoch 128 complete...\n",
      "Epoch 129: Train Loss 1.3545 Acc 61.12%\n",
      "Test Loss 1.1664 Acc 70.46%\n",
      "Epoch 129 complete...\n",
      "Epoch 130: Train Loss 1.3486 Acc 61.60%\n",
      "Test Loss 1.1704 Acc 69.89%\n",
      "Epoch 130 complete...\n",
      "Epoch 131: Train Loss 1.3469 Acc 61.71%\n",
      "Test Loss 1.1553 Acc 70.72%\n",
      "Epoch 131 complete...\n",
      "Epoch 132: Train Loss 1.3414 Acc 61.90%\n",
      "Test Loss 1.1562 Acc 70.77%\n",
      "Epoch 132 complete...\n",
      "Epoch 133: Train Loss 1.3359 Acc 62.01%\n",
      "Test Loss 1.1860 Acc 69.18%\n",
      "Epoch 133 complete...\n",
      "Epoch 134: Train Loss 1.3335 Acc 62.12%\n",
      "Test Loss 1.1558 Acc 70.49%\n",
      "Epoch 134 complete...\n",
      "Epoch 135: Train Loss 1.3297 Acc 62.21%\n",
      "Test Loss 1.1569 Acc 70.87%\n",
      "Epoch 135 complete...\n",
      "Epoch 136: Train Loss 1.3266 Acc 62.43%\n",
      "Test Loss 1.1652 Acc 70.62%\n",
      "Epoch 136 complete...\n",
      "Epoch 137: Train Loss 1.3276 Acc 62.54%\n",
      "Test Loss 1.1485 Acc 71.15%\n",
      "Epoch 137 complete...\n",
      "Epoch 138: Train Loss 1.3192 Acc 63.08%\n",
      "Test Loss 1.1735 Acc 70.28%\n",
      "Epoch 138 complete...\n",
      "Epoch 139: Train Loss 1.3151 Acc 63.11%\n",
      "Test Loss 1.1589 Acc 69.94%\n",
      "Epoch 139 complete...\n",
      "Epoch 140: Train Loss 1.3150 Acc 63.00%\n",
      "Test Loss 1.1294 Acc 72.10%\n",
      "Epoch 140 complete...\n",
      "Epoch 141: Train Loss 1.3141 Acc 62.88%\n",
      "Test Loss 1.1216 Acc 72.23%\n",
      "Epoch 141 complete...\n",
      "Epoch 142: Train Loss 1.3016 Acc 63.59%\n",
      "Test Loss 1.1245 Acc 71.75%\n",
      "Epoch 142 complete...\n",
      "Epoch 143: Train Loss 1.3021 Acc 63.60%\n",
      "Test Loss 1.1322 Acc 71.46%\n",
      "Epoch 143 complete...\n",
      "Epoch 144: Train Loss 1.2989 Acc 63.72%\n",
      "Test Loss 1.1150 Acc 72.51%\n",
      "Epoch 144 complete...\n",
      "Epoch 145: Train Loss 1.2936 Acc 64.09%\n",
      "Test Loss 1.1195 Acc 71.91%\n",
      "Epoch 145 complete...\n",
      "Epoch 146: Train Loss 1.2935 Acc 64.12%\n",
      "Test Loss 1.1218 Acc 72.08%\n",
      "Epoch 146 complete...\n",
      "Epoch 147: Train Loss 1.2870 Acc 64.22%\n",
      "Test Loss 1.1221 Acc 72.47%\n",
      "Epoch 147 complete...\n",
      "Epoch 148: Train Loss 1.2887 Acc 64.11%\n",
      "Test Loss 1.0929 Acc 73.62%\n",
      "Epoch 148 complete...\n",
      "Epoch 149: Train Loss 1.2785 Acc 64.92%\n",
      "Test Loss 1.1213 Acc 71.76%\n",
      "Epoch 149 complete...\n",
      "Epoch 150: Train Loss 1.2768 Acc 64.93%\n",
      "Test Loss 1.1090 Acc 72.89%\n",
      "Epoch 150 complete...\n",
      "Epoch 151: Train Loss 1.2718 Acc 65.19%\n",
      "Test Loss 1.1124 Acc 72.60%\n",
      "Epoch 151 complete...\n",
      "Epoch 152: Train Loss 1.2675 Acc 65.33%\n",
      "Test Loss 1.1017 Acc 73.16%\n",
      "Epoch 152 complete...\n",
      "Epoch 153: Train Loss 1.2696 Acc 65.26%\n",
      "Test Loss 1.0809 Acc 73.89%\n",
      "Epoch 153 complete...\n",
      "Epoch 154: Train Loss 1.2630 Acc 65.61%\n",
      "Test Loss 1.0745 Acc 74.18%\n",
      "Epoch 154 complete...\n",
      "Epoch 155: Train Loss 1.2590 Acc 65.65%\n",
      "Test Loss 1.0846 Acc 73.70%\n",
      "Epoch 155 complete...\n",
      "Epoch 156: Train Loss 1.2512 Acc 66.07%\n",
      "Test Loss 1.0733 Acc 74.27%\n",
      "Epoch 156 complete...\n",
      "Epoch 157: Train Loss 1.2475 Acc 66.23%\n",
      "Test Loss 1.0689 Acc 74.74%\n",
      "Epoch 157 complete...\n",
      "Epoch 158: Train Loss 1.2501 Acc 66.33%\n",
      "Test Loss 1.0951 Acc 73.74%\n",
      "Epoch 158 complete...\n",
      "Epoch 159: Train Loss 1.2481 Acc 66.20%\n",
      "Test Loss 1.0727 Acc 74.78%\n",
      "Epoch 159 complete...\n",
      "Epoch 160: Train Loss 1.2422 Acc 66.25%\n",
      "Test Loss 1.1028 Acc 73.03%\n",
      "Epoch 160 complete...\n",
      "Epoch 161: Train Loss 1.2379 Acc 66.55%\n",
      "Test Loss 1.0696 Acc 74.68%\n",
      "Epoch 161 complete...\n",
      "Epoch 162: Train Loss 1.2374 Acc 66.42%\n",
      "Test Loss 1.0819 Acc 73.54%\n",
      "Epoch 162 complete...\n",
      "Epoch 163: Train Loss 1.2338 Acc 66.68%\n",
      "Test Loss 1.0714 Acc 74.17%\n",
      "Epoch 163 complete...\n",
      "Epoch 164: Train Loss 1.2315 Acc 66.86%\n",
      "Test Loss 1.0804 Acc 73.91%\n",
      "Epoch 164 complete...\n",
      "Epoch 165: Train Loss 1.2279 Acc 67.08%\n",
      "Test Loss 1.0468 Acc 75.47%\n",
      "Epoch 165 complete...\n",
      "Epoch 166: Train Loss 1.2238 Acc 67.16%\n",
      "Test Loss 1.0463 Acc 75.35%\n",
      "Epoch 166 complete...\n",
      "Epoch 167: Train Loss 1.2202 Acc 67.46%\n",
      "Test Loss 1.0445 Acc 75.56%\n",
      "Epoch 167 complete...\n",
      "Epoch 168: Train Loss 1.2099 Acc 67.76%\n",
      "Test Loss 1.0641 Acc 74.69%\n",
      "Epoch 168 complete...\n",
      "Epoch 169: Train Loss 1.2114 Acc 67.72%\n",
      "Test Loss 1.0548 Acc 75.55%\n",
      "Epoch 169 complete...\n",
      "Epoch 170: Train Loss 1.2122 Acc 67.92%\n",
      "Test Loss 1.0418 Acc 75.84%\n",
      "Epoch 170 complete...\n",
      "Epoch 171: Train Loss 1.2069 Acc 68.08%\n",
      "Test Loss 1.0588 Acc 75.33%\n",
      "Epoch 171 complete...\n",
      "Epoch 172: Train Loss 1.2059 Acc 68.14%\n",
      "Test Loss 1.0322 Acc 76.51%\n",
      "Epoch 172 complete...\n",
      "Epoch 173: Train Loss 1.1974 Acc 68.47%\n",
      "Test Loss 1.0299 Acc 76.12%\n",
      "Epoch 173 complete...\n",
      "Epoch 174: Train Loss 1.1928 Acc 68.79%\n",
      "Test Loss 1.0349 Acc 76.35%\n",
      "Epoch 174 complete...\n",
      "Epoch 175: Train Loss 1.1954 Acc 68.60%\n",
      "Test Loss 1.0208 Acc 76.57%\n",
      "Epoch 175 complete...\n",
      "Epoch 176: Train Loss 1.1920 Acc 68.62%\n",
      "Test Loss 1.0272 Acc 76.61%\n",
      "Epoch 176 complete...\n",
      "Epoch 177: Train Loss 1.1926 Acc 68.74%\n",
      "Test Loss 1.0395 Acc 75.85%\n",
      "Epoch 177 complete...\n",
      "Epoch 178: Train Loss 1.1847 Acc 69.01%\n",
      "Test Loss 1.0186 Acc 77.07%\n",
      "Epoch 178 complete...\n",
      "Epoch 179: Train Loss 1.1821 Acc 69.31%\n",
      "Test Loss 1.0327 Acc 76.51%\n",
      "Epoch 179 complete...\n",
      "Epoch 180: Train Loss 1.1771 Acc 69.41%\n",
      "Test Loss 1.0120 Acc 77.28%\n",
      "Epoch 180 complete...\n",
      "Epoch 181: Train Loss 1.1758 Acc 69.48%\n",
      "Test Loss 1.0082 Acc 77.52%\n",
      "Epoch 181 complete...\n",
      "Epoch 182: Train Loss 1.1679 Acc 69.82%\n",
      "Test Loss 1.0286 Acc 76.54%\n",
      "Epoch 182 complete...\n",
      "Epoch 183: Train Loss 1.1635 Acc 70.13%\n",
      "Test Loss 1.0260 Acc 76.62%\n",
      "Epoch 183 complete...\n",
      "Epoch 184: Train Loss 1.1728 Acc 69.62%\n",
      "Test Loss 1.0023 Acc 77.41%\n",
      "Epoch 184 complete...\n",
      "Epoch 185: Train Loss 1.1634 Acc 70.11%\n",
      "Test Loss 1.0064 Acc 77.47%\n",
      "Epoch 185 complete...\n",
      "Epoch 186: Train Loss 1.1597 Acc 70.38%\n",
      "Test Loss 1.0089 Acc 77.57%\n",
      "Epoch 186 complete...\n",
      "Epoch 187: Train Loss 1.1546 Acc 70.52%\n",
      "Test Loss 0.9962 Acc 78.16%\n",
      "Epoch 187 complete...\n",
      "Epoch 188: Train Loss 1.1543 Acc 70.62%\n",
      "Test Loss 0.9965 Acc 78.19%\n",
      "Epoch 188 complete...\n",
      "Epoch 189: Train Loss 1.1527 Acc 70.43%\n",
      "Test Loss 1.0003 Acc 77.89%\n",
      "Epoch 189 complete...\n",
      "Epoch 190: Train Loss 1.1474 Acc 70.80%\n",
      "Test Loss 0.9914 Acc 78.38%\n",
      "Epoch 190 complete...\n",
      "Epoch 191: Train Loss 1.1440 Acc 70.93%\n",
      "Test Loss 1.0121 Acc 77.02%\n",
      "Epoch 191 complete...\n",
      "Epoch 192: Train Loss 1.1409 Acc 70.87%\n",
      "Test Loss 0.9844 Acc 78.43%\n",
      "Epoch 192 complete...\n",
      "Epoch 193: Train Loss 1.1397 Acc 71.07%\n",
      "Test Loss 0.9936 Acc 77.84%\n",
      "Epoch 193 complete...\n",
      "Epoch 194: Train Loss 1.1424 Acc 70.92%\n",
      "Test Loss 1.0028 Acc 77.19%\n",
      "Epoch 194 complete...\n",
      "Epoch 195: Train Loss 1.1305 Acc 71.40%\n",
      "Test Loss 0.9999 Acc 77.42%\n",
      "Epoch 195 complete...\n",
      "Epoch 196: Train Loss 1.1368 Acc 71.36%\n",
      "Test Loss 0.9683 Acc 79.46%\n",
      "Epoch 196 complete...\n",
      "Epoch 197: Train Loss 1.1317 Acc 71.52%\n",
      "Test Loss 0.9822 Acc 78.75%\n",
      "Epoch 197 complete...\n",
      "Epoch 198: Train Loss 1.1271 Acc 71.86%\n",
      "Test Loss 1.0021 Acc 77.92%\n",
      "Epoch 198 complete...\n",
      "Epoch 199: Train Loss 1.1193 Acc 71.96%\n",
      "Test Loss 0.9722 Acc 79.39%\n",
      "Epoch 199 complete...\n",
      "Epoch 200: Train Loss 1.1161 Acc 72.20%\n",
      "Test Loss 0.9642 Acc 79.47%\n",
      "Epoch 200 complete...\n",
      "Best test acc: 79.47\n"
     ]
    }
   ],
   "source": [
    "best_acc = 0.\n",
    "for epoch in range(1, epochs+1):\n",
    "    train_one_epoch(epoch,lr_scheduler)\n",
    "    acc = evaluate()\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "\n",
    "        torch.save(model.state_dict(), \"best_vit_cifar10.pth\")\n",
    "    print(f\"Epoch {epoch} complete...\")\n",
    "print(\"Best test acc:\", best_acc)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
